{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"TSQes2nq06lK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702109451621,"user_tz":-540,"elapsed":25159,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"3de0d527-496f-446a-c8fb-ec8f4e59c470"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702109451622,"user":{"displayName":"최유민","userId":"14691279395225646105"},"user_tz":-540},"id":"PmrNOnmYT2D3"},"outputs":[],"source":["import os\n"]},{"cell_type":"markdown","metadata":{"id":"Eyhuu65QUTIy"},"source":["### Prepare Dataset"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":8,"status":"error","timestamp":1702030737930,"user":{"displayName":"최유민","userId":"14691279395225646105"},"user_tz":-540},"id":"V8fblLAZ8WzJ","colab":{"base_uri":"https://localhost:8080/","height":345},"outputId":"abd35a3f-4adc-453a-f76f-f9379296e643"},"outputs":[{"output_type":"error","ename":"FileExistsError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-6abcbc06749f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# download coco dataset in colab runtime memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/coco'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/coco/annotations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data/coco'"]}],"source":["# download coco dataset in colab runtime memory\n","os.makedirs('data/coco')\n","os.makedirs('data/coco/annotations')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1702030737930,"user":{"displayName":"최유민","userId":"14691279395225646105"},"user_tz":-540},"id":"kj4671ZYUGAh"},"outputs":[],"source":["cd data/coco"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHQloGnH2GKq","executionInfo":{"status":"aborted","timestamp":1702030737931,"user_tz":-540,"elapsed":7,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["!wget http://images.cocodataset.org/zips/train2017.zip\n","!wget http://images.cocodataset.org/zips/val2017.zip\n","#!wget http://images.cocodataset.org/zips/test2017.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbwjF28Z98v1","executionInfo":{"status":"aborted","timestamp":1702030737931,"user_tz":-540,"elapsed":7,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["!unzip -qq \"/content/data/coco/train2017.zip\"\n","!unzip -qq \"/content/data/coco/val2017.zip\"\n","#!unzip -qq \"/content/coco/test2017.zip\"\n"]},{"cell_type":"markdown","metadata":{"id":"Q-8VyjRJUXgj"},"source":["### CLIP"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Q8Oi_vVv1BhD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702082888492,"user_tz":-540,"elapsed":1132,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"c3789ac5-93a3-4b1f-d70f-be306d21ba5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption\n"]}],"source":["cd /content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"UxrPuX2hR4z0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702019552072,"user_tz":-540,"elapsed":9789,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"e2d980fe-ed76-4732-a125-744d5bb2ca36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai-clip\n","  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from openai-clip)\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from openai-clip) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-clip) (4.66.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->openai-clip) (0.2.12)\n","Building wheels for collected packages: openai-clip\n","  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=8bfa8851e81f5b1a69891b2b9b05629f095cb13da8c814a566b5fb9fc120a7f3\n","  Stored in directory: /root/.cache/pip/wheels/08/77/8e/8d2f862df6bf7fb4e2007062d2cbaeae49862ec7b56d041229\n","Successfully built openai-clip\n","Installing collected packages: ftfy, openai-clip\n","Successfully installed ftfy-6.1.3 openai-clip-1.0.1\n"]}],"source":["!pip install openai-clip"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Hry2U_aXa6-d","executionInfo":{"status":"ok","timestamp":1702019559488,"user_tz":-540,"elapsed":7421,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["import clip"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Vrx8Ti26RYKb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702019559488,"user_tz":-540,"elapsed":6,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"b759ccdc-8b1f-4019-f55c-c35a94d435b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['RN50',\n"," 'RN101',\n"," 'RN50x4',\n"," 'RN50x16',\n"," 'RN50x64',\n"," 'ViT-B/32',\n"," 'ViT-B/16',\n"," 'ViT-L/14',\n"," 'ViT-L/14@336px']"]},"metadata":{},"execution_count":10}],"source":["clip.available_models()"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"-RIWBl8aLn-F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702035285815,"user_tz":-540,"elapsed":4505229,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"4177e93c-90b3-4f13-b613-3005ed7f874d"},"outputs":[{"output_type":"stream","name":"stdout","text":["566747 captions loaded from json \n","checkpoint loaded\n","checkpoint shape: torch.Size([360000, 512])\n","  5% 9999/206747 [03:32<56:59, 57.54it/s]index 369999 saved\n"," 10% 19996/206747 [07:01<1:24:06, 37.00it/s]index 379999 saved\n"," 15% 29997/206747 [10:29<50:54, 57.86it/s]index 389999 saved\n"," 19% 39996/206747 [14:04<1:12:58, 38.09it/s]index 399999 saved\n"," 24% 49998/206747 [17:38<1:12:51, 35.86it/s]index 409999 saved\n"," 29% 59997/206747 [21:18<44:07, 55.42it/s]index 419999 saved\n"," 34% 69997/206747 [24:55<57:41, 39.51it/s]index 429999 saved\n"," 39% 79999/206747 [28:25<56:08, 37.63it/s]index 439999 saved\n"," 44% 89998/206747 [32:07<35:56, 54.13it/s]index 449999 saved\n"," 48% 99999/206747 [35:41<32:07, 55.39it/s]index 459999 saved\n"," 53% 109996/206747 [39:29<51:16, 31.45it/s]index 469999 saved\n"," 58% 119998/206747 [43:15<25:37, 56.43it/s]index 479999 saved\n"," 63% 129997/206747 [46:53<33:14, 38.49it/s]index 489999 saved\n"," 68% 139996/206747 [50:34<19:14, 57.83it/s]index 499999 saved\n"," 73% 149995/206747 [54:10<16:52, 56.03it/s]index 509999 saved\n"," 77% 159997/206747 [57:44<14:19, 54.40it/s]index 519999 saved\n"," 82% 169999/206747 [1:01:19<12:08, 50.41it/s]index 529999 saved\n"," 87% 179997/206747 [1:04:53<07:42, 57.80it/s]index 539999 saved\n"," 92% 189996/206747 [1:08:26<04:49, 57.86it/s]index 549999 saved\n"," 97% 199999/206747 [1:12:01<02:51, 39.39it/s]index 559999 saved\n","100% 206747/206747 [1:14:27<00:00, 46.28it/s]\n","index 566746 saved\n","Done\n","566747 embeddings saved \n"]}],"source":["# Pre Trained 된 CLIP 모델을 이용, 이미지를 CLIP Encoder 통과\n","# clip model ViT-B/32 is used in ClipCap paper\n","\n","!python parse_coco.py --clip_model_type ViT-B/32 --data_path '/content/data/coco' --out_path '/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/data/coco' --start_index 360000"]},{"cell_type":"markdown","metadata":{"id":"CVinYb46-93C"},"source":["### Train Prefix Network"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eKvzcAkVC-A_","executionInfo":{"status":"ok","timestamp":1702109453237,"user_tz":-540,"elapsed":1619,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"acf4e799-bd8a-4b38-d950-c0dcfa0f97ff"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uuEK8pb-LoAM","executionInfo":{"status":"ok","timestamp":1702113894856,"user_tz":-540,"elapsed":3866229,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"70128d74-08c9-4591-eea9-b0502becbb1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["vocab.json: 100% 1.04M/1.04M [00:00<00:00, 3.19MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 38.0MB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.31MB/s]\n","config.json: 100% 665/665 [00:00<00:00, 3.08MB/s]\n","Data size is 566747\n","model.safetensors: 100% 548M/548M [00:01<00:00, 290MB/s]\n","generation_config.json: 100% 124/124 [00:00<00:00, 610kB/s]\n","Train only prefix\n","loading model from /content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/coco_train/coco_prefix-001.pt\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n",">>> Training epoch 0\n","coco_prefix: 100% 8960/8960 [1:07:30<00:00,  2.21it/s, loss=5.15]\n",">>> Training epoch 1\n","coco_prefix:   6% 562/8960 [04:13<1:03:02,  2.22it/s, loss=5.2]Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/train.py\", line 416, in <module>\n","    main()\n","  File \"/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/train.py\", line 412, in main\n","    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)\n","  File \"/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/train.py\", line 353, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","coco_prefix:   6% 562/8960 [04:14<1:03:21,  2.21it/s, loss=5.2]\n"]}],"source":["!python train.py --only_prefix --quantisation --prefix_length 40 --bs 64 --data ./data/coco/oscar_split_ViT-B_32_train.pkl --out_dir ./coco_train/ --mapping_type transformer --model_path '/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/coco_train/coco_prefix-001.pt'"]},{"cell_type":"code","source":["!python train.py --only_prefix --bs 64 --data ./data/coco/oscar_split_ViT-B_32_train.pkl --out_dir ./coco_train/ --mapping_type transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqBJDnY6oKp8","executionInfo":{"status":"ok","timestamp":1702096995196,"user_tz":-540,"elapsed":10930909,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"4be0c082-1253-4abc-e241-08ecfa574c71"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Data size is 566747\n","Train only prefix\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n",">>> Training epoch 0\n","coco_prefix: 100% 8960/8960 [41:07<00:00,  3.63it/s, loss=2.68]\n",">>> Training epoch 1\n","coco_prefix: 100% 8960/8960 [41:02<00:00,  3.64it/s, loss=2.41]\n",">>> Training epoch 2\n","coco_prefix: 100% 8960/8960 [40:55<00:00,  3.65it/s, loss=2.01]\n",">>> Training epoch 3\n","coco_prefix: 100% 8960/8960 [40:55<00:00,  3.65it/s, loss=2.39]\n",">>> Training epoch 4\n","coco_prefix:  39% 3507/8960 [16:02<24:51,  3.66it/s, loss=2.36]Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/train.py\", line 408, in <module>\n","    main()\n","  File \"/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/train.py\", line 404, in main\n","    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)\n","  File \"/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/train.py\", line 350, in train\n","    outputs = model(tokens, prefix, mask)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/Colab Notebooks/DeepLearning/DL_final_project/DL-final_project/CLIP_prefix_caption/train.py\", line 265, in forward\n","    out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n","    transformer_outputs = self.transformer(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n","    outputs = block(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n","    attn_outputs = self.attn(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n","    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 201, in _attn\n","    mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n","KeyboardInterrupt\n","coco_prefix:  39% 3507/8960 [16:03<24:57,  3.64it/s, loss=2.36]\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyOzeavN2mk0NsJBu0jdvstO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}